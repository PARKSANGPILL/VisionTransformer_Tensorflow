{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2569d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88789f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "patch_size = 16\n",
    "num_patches = input_shape[0] * input_shape[1] // patch_size ** 2\n",
    "num_heads = 8\n",
    "projection_dim = 512\n",
    "drop_rate = 0.3\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36551a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patchs, patch_size, projection_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.pos = self.add_weight(shape=[num_patches, projection_dim], initializer='normal', dtype=tf.float32, trainable=True)\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=patch_size**2 * 3, kernel_size=patch_size, strides=patch_size)\n",
    "        self.linear_proj = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = self.conv(x)\n",
    "        x = tf.reshape(x, shape=[batch_size, -1, self.patch_size**2 * 3])\n",
    "        x = self.linear_proj(x)\n",
    "        x = x + self.pos\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fc6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.Model):\n",
    "    def __init__(self, num_heads, projection_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = projection_dim\n",
    "        self.w = tf.keras.layers.Dense(units=projection_dim)\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        query = self.w(x)\n",
    "        query = tf.reshape(query, shape=[batch_size, -1, self.num_heads, self.projection_dim // self.num_heads])\n",
    "        query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        key = self.w(x)\n",
    "        key = tf.reshape(query, shape=[batch_size, -1, self.num_heads, self.projection_dim // self.num_heads])\n",
    "        key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        value = self.w(x)\n",
    "        value = tf.reshape(query, shape=[batch_size, -1, self.num_heads, self.projection_dim // self.num_heads])\n",
    "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        attention = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(tf.shape(key)[-1], dtype=tf.float32))\n",
    "        \n",
    "        x = self.softmax(attention)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.matmul(x, value)\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        x = tf.reshape(x, shape=[batch_size, -1, self.projection_dim])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef65793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 256, 512)     1115392     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 256, 512)    1024        ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_self_attention (Mul  (None, None, 512)   262656      ['layer_normalization[0][0]']    \n",
      " tiHeadSelfAttention)                                                                             \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 256, 512)     0           ['multi_head_self_attention[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 256, 512)    1024        ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 256, 1024)    525312      ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 256, 1024)    0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256, 512)     524800      ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 256, 512)     0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256, 512)     0           ['dropout_3[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 256, 512)    1024        ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_self_attention_1 (M  (None, None, 512)   262656      ['layer_normalization_2[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 256, 512)     0           ['multi_head_self_attention_1[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 256, 512)    1024        ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 256, 1024)    525312      ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 256, 1024)    0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 256, 512)     524800      ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 256, 512)     0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 256, 512)     0           ['dropout_6[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 256, 512)    1024        ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_self_attention_2 (M  (None, None, 512)   262656      ['layer_normalization_4[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 256, 512)     0           ['multi_head_self_attention_2[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 256, 512)    1024        ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 256, 1024)    525312      ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 256, 1024)    0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256, 512)     524800      ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 256, 512)     0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 256, 512)     0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 256, 512)    1024        ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " multi_head_self_attention_3 (M  (None, None, 512)   262656      ['layer_normalization_6[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 256, 512)     0           ['multi_head_self_attention_3[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 256, 512)    1024        ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 256, 1024)    525312      ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 256, 1024)    0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 256, 512)     524800      ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256, 512)     0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 256, 512)     0           ['dropout_12[0][0]',             \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 256, 512)    1024        ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_self_attention_4 (M  (None, None, 512)   262656      ['layer_normalization_8[0][0]']  \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 256, 512)     0           ['multi_head_self_attention_4[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 256, 512)    1024        ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 256, 1024)    525312      ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256, 1024)    0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 256, 512)     524800      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 256, 512)     0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 256, 512)     0           ['dropout_15[0][0]',             \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 256, 512)    1024        ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_self_attention_5 (M  (None, None, 512)   262656      ['layer_normalization_10[0][0]'] \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 256, 512)     0           ['multi_head_self_attention_5[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 256, 512)    1024        ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 256, 1024)    525312      ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 256, 1024)    0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 256, 512)     524800      ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 256, 512)     0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 256, 512)     0           ['dropout_18[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 256, 512)    1024        ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_self_attention_6 (M  (None, None, 512)   262656      ['layer_normalization_12[0][0]'] \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 256, 512)     0           ['multi_head_self_attention_6[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 256, 512)    1024        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 256, 1024)    525312      ['layer_normalization_13[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 256, 1024)    0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 256, 512)     524800      ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 256, 512)     0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 256, 512)     0           ['dropout_21[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 256, 512)    1024        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_self_attention_7 (M  (None, None, 512)   262656      ['layer_normalization_14[0][0]'] \n",
      " ultiHeadSelfAttention)                                                                           \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 256, 512)     0           ['multi_head_self_attention_7[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 256, 512)    1024        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 256, 1024)    525312      ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 256, 1024)    0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 256, 512)     524800      ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 256, 512)     0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 256, 512)     0           ['dropout_24[0][0]',             \n",
      "                                                                  'add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_16 (LayerN  (None, 256, 512)    1024        ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 512)         0           ['layer_normalization_16[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 512)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 1024)         525312      ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 1024)         0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 100)          102500      ['dropout_26[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,262,756\n",
      "Trainable params: 12,262,756\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = PatchEncoder(num_patches, patch_size, projection_dim, drop_rate)(inputs)\n",
    "    \n",
    "    for _ in range(8):\n",
    "        x1 = tf.keras.layers.LayerNormalization()(x)\n",
    "        x1 = MultiHeadSelfAttention(num_heads, projection_dim, drop_rate)(x1)\n",
    "        x2 = tf.keras.layers.Add()([x1, x])\n",
    "        x3 = tf.keras.layers.LayerNormalization()(x2)\n",
    "        x3 = tf.keras.layers.Dense(projection_dim * 2, activation=tf.nn.gelu)(x3)\n",
    "        x3 = tf.keras.layers.Dropout(drop_rate)(x3)\n",
    "        x3 = tf.keras.layers.Dense(projection_dim, activation=tf.nn.gelu)(x3)\n",
    "        x3 = tf.keras.layers.Dropout(drop_rate)(x3)\n",
    "        x = tf.keras.layers.Add()([x3, x2])\n",
    "        \n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rate)(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=tf.nn.gelu)(x)\n",
    "    x = tf.keras.layers.Dropout(drop_rate)(x)\n",
    "    x = tf.keras.layers.Dense(num_classes)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "model = model()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
